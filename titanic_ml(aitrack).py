# -*- coding: utf-8 -*-
"""Titanic_ML(AItrack).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rIB1_3CtPVY5kOsqEEm9QfgQzk5H-eOs

# Titanic - Machine Learning

This project aims to introduce the most important steps of data analysis and explore the different stages. We will use the data of Titanic survivors available on the Kaggle website at the following link:
https://www.kaggle.com/competitions/titanic/overview

You can download the dataset and explore all the information about it in the following link: https://www.kaggle.com/competitions/titanic/data

Note: To complete this projct, you need to modify the cells that contain the code below before submitting the project. All other cells should remain unchanged without any modifications.

############################ <br>
أكمل الكود <br>
Complete the code <br>
############################ <br>

## Importing the Dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

"""## Reading the data

We will first use `pd.read_csv` to load the data from csv file to Pandas DataFrame:
"""

data = pd.read_csv('/content/train.csv')

"""After reading the data, we will now review the data to ensure it has been read correctly by using the command `head`:"""

#this will print first 5 rows in the dataset
data.head()

# number of rows and columns
data.shape

"""## Data Preprocessing

Now we will use the info command to learn more about the data, such as the number of rows and columns, data types, and the number of missing values.
"""

data.info()

"""### Dealing with Missing Data"""

# to view the Missing valuse in each column:
data.isnull().sum()

"""You have three options to fix this:

*   Delete rows that contains missing valuse
*   Delete the whole column that contains missing values
*   Replace missing values with some value (Mean, Median, Mode, constant)

There are three columns contains Missing values: **Age, Cabin, Embarked**. <br>
In the Age column, we will fill the missing values with the mean since it is a simple and quick method to handle missing data and helps maintain the overall distribution of the dataset.
"""

############################
#أكمل الكود
#Complete the code
############################

#fill the missing values in Age with the mean of Age column
#you can simply use 'filllna' function, or any other way such as SimpleImputer
mean_age = data['Age'].mean()
data['Age'].fillna(mean_age, inplace=True)

data['Age'].isnull().sum()

"""There are a large number of missing values in the Cabin column, so we will drop this column from the dataset."""

data = data.drop(['Cabin'], axis=1)

data.head()

"""In the Embarked column, there are only two missing values. Let's see what the categories in this column are."""

data['Embarked'].value_counts()

############################
#أكمل الكود
#Complete the code
############################

#fill the missing values in Embarked with the mode of Embarked column:
most_frequent_embarked = data['Embarked'].mode()[0]
data['Embarked'].fillna(most_frequent_embarked, inplace=True)

data['Embarked'].isnull().sum()

"""### Drop useless columns

As you know, the PassengerId and Name of the Passenger do not affect the probability of survival. and ticket column does not have a clear relationship to the survival of passengers, so they will be dropped:
"""

############################
#أكمل الكود
#Complete the code
############################

# Drop the PassengerId and Name Columns from the dataset:

data = data.drop(['PassengerId'], axis=1)

data = data.drop(['Name'], axis=1)


data = data.drop(['Ticket'], axis=1)

data.head()

"""### Encode Categorical Columns

Sex and Embarked columns values are text, we can't give this text directly to the machine learning model, so we need to replace this text values to meaningful numerical values.

In Age column we will replace all male values with 0 and all the female values with 1. <br>
and we will do the same in Embarked column: S=> 0 , C=> 1, Q => 2
"""

data.replace({'Sex':{'male':0,'female':1},'Embarked':{'S':0,'C':1,'Q':2}}, inplace=True)

data.head()

"""### Dealing with Duplicates

Check if there are duplicates in the dataset:
"""

############################
#أكمل الكود
#Complete the code
############################

#check if there are duplicates in the dataset:
data.duplicated().sum()

############################
#أكمل الكود
#Complete the code
############################

#drop the duplicates:
data.drop_duplicates(inplace=True)

data.duplicated().any()

"""### Data Analysis

In this section, we will explore the data and the relationships between features using statistical analysis and visualization techniques. This will help us understand the underlying patterns and correlations in the dataset, providing valuable insights for model building.

describe() provides summary statistics for numerical columns, including count, mean, standard deviation, min, max, and quartiles. This function helps us understand the distribution and central tendencies of the data. However, in our Titanic dataset, while useful, it may not be the primary focus since many insights come from categorical features and their relationships with survival, which are better explored through other means.
"""

data.describe()

"""**Look for Correlations**

Now to understand the relations between the features we can use the **correlation matrix** which shows the correlation coefficients between different features in a dataset. Each cell in the matrix represents the correlation between two features. The correlation coefficient ranges from **-1 to 1**, where:<br>

1 indicates a perfect positive correlation: as one feature increases, the other feature increases proportionally. <br>

-1 indicates a perfect negative correlation: as one feature increases, the other feature decreases proportionally.<br>

0 indicates no correlation: the features do not show any linear relationship.<br>
"""

non_numeric_columns = data.select_dtypes(exclude=[float, int]).columns
print(f'Non-numeric columns: {non_numeric_columns}')

data.corr()['Survived']

"""The correlation values provide insights into how different features relate to the survival outcome in the Titanic dataset:

* Pclass: Negative correlation (-0.338). Higher classes (lower number) are more
likely to survive.
* Sex: Positive correlation (0.543). Females are more likely to survive.
* Age: Slight negative correlation (-0.070). Older passengers have a marginally lower chance of survival.
* SibSp: Slight negative correlation (-0.035). Having more siblings/spouses aboard slightly decreases survival chances.
* Parch: Slight positive correlation (0.082). Having more parents/children aboard slightly increases survival chances.
* Fare: Positive correlation (0.257). Passengers who paid higher fares are more likely to survive.
* Embarked: Slight positive correlation (0.107). The port of embarkation has a minor effect on survival.<br>
These correlations help identify which features may be important for predicting survival.
"""

# to understand more about data lets find the number of people survived and not survived
data['Survived'].value_counts()

# making a count plot for 'Survived' column
sns.countplot(x='Survived', data=data)

# making a count plot for 'Sex' column
sns.countplot(x='Sex', data=data)

# now lets compare the number of survived beasd on the gender
sns.countplot(x='Sex', hue='Survived', data=data)

"""as we can see, even we have more number of male in our dataset, the number of fmale who have survived is more. this is one of the very important insight that we can get from this data."""

# now lets compare the number of survived beasd on the Pclass
sns.countplot(x='Pclass', hue='Survived', data=data)

"""You can do the same for the other columns to get more insights about the dataset.

## Model Building

**Separating features & Target** <br><br>
Separating features and target so that we can prepare the data for training machine learning models. In the Titanic dataset, the Survived column is the target variable, and the other columns are the features.
"""

x = data.drop(columns = ['Survived'], axis=1)
y = data['Survived']

"""**Splitting the data into training data & Testing data**

To build and evaluate a machine learning model effectively, it's essential to split the dataset into training and testing sets. The training set is used to train the model, allowing it to learn patterns and relationships within the data. The testing set, on the other hand, is used to evaluate the model's performance on unseen data, ensuring it can generalize well to new instances. This split helps prevent overfitting and provides a reliable estimate of the model's predictive accuracy.
"""

############################
#أكمل الكود
#Complete the code
############################

from sklearn.model_selection import train_test_split

# Split the data into training data & Testing data using train_test_split function :


X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)

"""**Model Training**

Model training is a crucial step in the machine learning where the algorithm learns from the training data to make predictions. **Logistic Regression** is a commonly used algorithm for binary classification tasks, such as predicting whether a passenger survived in the Titanic dataset. By training the model on our training data, we aim to find the best-fit parameters that minimize prediction errors. Once trained, this model can be used to predict outcomes on new, unseen data.
"""

data.head()

############################
#أكمل الكود
#Complete the code
############################

from sklearn.linear_model import LogisticRegression

# Create a Logistic Regression model and Train it on the training data:

logistic_model = LogisticRegression()


logistic_model.fit(x, y)

"""## Model Evaluation

Model evaluation is crucial in machine learning to assess the performance of a trained model on testing data. The **accuracy score**, a common evaluation metric, measures the proportion of correct predictions out of all predictions. This helps to gauge the model's effectiveness, ensure it generalizes well to new data, and guide further improvements.
"""

############################
#أكمل الكود
#Complete the code
############################

from sklearn.metrics import accuracy_score

#first let the model predict x_test
#then use accuracy score to see the accuracy of the model
#finally print the Accuracy.


# Predict the labels for the test data
y_pred = logistic_model.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy score
print(f'Accuracy: {accuracy:.4f}')